# Vision Transformer (ViT) å®ç°

è¿™æ˜¯ä¸€ä¸ªåŸºäºPyTorchçš„Vision Transformerï¼ˆViTï¼‰å®Œæ•´å®ç°ï¼Œæ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å’ŒCIFAR-10æ•°æ®é›†çš„è®­ç»ƒä¸æ¨ç†ã€‚

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

Vision Transformer (ViT) æ˜¯Googleåœ¨2020å¹´æå‡ºçš„çº¯Transformeræ¶æ„ç”¨äºå›¾åƒåˆ†ç±»çš„æ¨¡å‹ã€‚æœ¬é¡¹ç›®å®ç°äº†æ ‡å‡†çš„ViTæ¶æ„ï¼ŒåŒ…å«ï¼š

- **Patch Embedding**: å°†å›¾åƒåˆ‡åˆ†ä¸ºpatcheså¹¶è¿›è¡Œçº¿æ€§æ˜ å°„
- **Multi-Head Self-Attention**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- **MLP**: å‰é¦ˆç¥ç»ç½‘ç»œ
- **Position Encoding**: ä½ç½®ç¼–ç 
- **Classification Token**: åˆ†ç±»token


## ç¯å¢ƒä¾èµ–

```bash
pip install torch torchvision tqdm matplotlib pillow numpy
```

æˆ–ä½¿ç”¨condaï¼š
```bash
conda install pytorch torchvision tqdm matplotlib pillow numpy -c pytorch
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. è‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒ

#### æ•°æ®é›†æ ¼å¼
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class1/
â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â””â”€â”€ img2.jpg
â”‚   â””â”€â”€ class2/
â”‚       â”œâ”€â”€ img3.jpg
â”‚       â””â”€â”€ img4.jpg
â””â”€â”€ val/
    â”œâ”€â”€ class1/
    â”‚   â””â”€â”€ img5.jpg
    â””â”€â”€ class2/
        â””â”€â”€ img6.jpg
```

#### è®­ç»ƒ
```bash
python3 train.py
```

#### é¢„æµ‹
```bash
python3 predict.py <å›¾ç‰‡è·¯å¾„>
```

### 2. CIFAR-10æ•°æ®é›†è®­ç»ƒ

#### ä¸‹è½½CIFAR-10æ•°æ®é›†
ä»[å®˜ç½‘](https://www.cs.toronto.edu/~kriz/cifar.html)ä¸‹è½½CIFAR-10 Pythonç‰ˆæœ¬ï¼Œè§£å‹åˆ°æŒ‡å®šç›®å½•ã€‚

#### è®­ç»ƒ
```bash
python3 train_cifar10.py
```

#### é¢„æµ‹
```bash
# å•ä¸ªé¢„æµ‹
python3 predict_cifar10.py <å›¾ç‰‡è·¯å¾„>

# Top-Ké¢„æµ‹
python3 predict_cifar10.py <å›¾ç‰‡è·¯å¾„> --top-k 3
```

## âš™ï¸ æ¨¡å‹é…ç½®

### é»˜è®¤å‚æ•°ï¼ˆViT-Baseï¼‰
- **å›¾åƒå°ºå¯¸**: 224Ã—224
- **Patchå°ºå¯¸**: 16Ã—16
- **åµŒå…¥ç»´åº¦**: 768
- **ç¼–ç å™¨å±‚æ•°**: 12
- **æ³¨æ„åŠ›å¤´æ•°**: 12
- **MLPæ¯”ä¾‹**: 4.0
- **Dropoutç‡**: 0.1

### è‡ªå®šä¹‰å‚æ•°
å¯åœ¨è®­ç»ƒè„šæœ¬ä¸­ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š
```python
train_cifar10(
    img_size=224,        # å›¾åƒå°ºå¯¸
    patch_size=16,       # Patchå°ºå¯¸
    num_features=768,    # åµŒå…¥ç»´åº¦
    depth=12,            # ç¼–ç å™¨å±‚æ•°
    num_heads=12,        # æ³¨æ„åŠ›å¤´æ•°
    mlp_ratio=4.0,       # MLPæ¯”ä¾‹
    epochs=50,           # è®­ç»ƒè½®æ•°
    batch_size=32,       # æ‰¹æ¬¡å¤§å°
    lr=3e-4             # å­¦ä¹ ç‡
)
```

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **VisionPatchEmbedding**: 
   - ä½¿ç”¨å·ç§¯å±‚å°†å›¾åƒåˆ‡åˆ†ä¸ºpatches
   - çº¿æ€§æ˜ å°„åˆ°åµŒå…¥ç©ºé—´
   - LayerNormå½’ä¸€åŒ–

2. **SelfAttention**:
   - å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
   - æ”¯æŒQKVåç½®
   - Dropoutæ­£åˆ™åŒ–

3. **MLP**:
   - ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
   - GELUæ¿€æ´»å‡½æ•°
   - Dropoutæ­£åˆ™åŒ–

4. **Block**:
   - Pre-Normç»“æ„
   - æ®‹å·®è¿æ¥
   - DropPathéšæœºæ·±åº¦

5. **VisonTransformer**:
   - ä½ç½®ç¼–ç æ’å€¼
   - åˆ†ç±»token
   - å¤šå±‚Transformerç¼–ç å™¨

### å‚æ•°é‡
- **ViT-Base**: ~86Må‚æ•°
- **ViT-Small**: ~22Må‚æ•°
- **ViT-Large**: ~307Må‚æ•°
## ğŸ“ è®­ç»ƒæ—¥å¿—

è®­ç»ƒè¿‡ç¨‹ä¼šè‡ªåŠ¨ä¿å­˜ï¼š
- æŸå¤±æ›²çº¿å›¾ (`loss_curve.png` / `cifar10_training_curves.png`)
- å‡†ç¡®ç‡æ›²çº¿å›¾ (`acc_curve.png`)
- æœ€ä¼˜æ¨¡å‹æƒé‡ (`best_vit.pth` / `best_vit_cifar10.pth`)



## ğŸ“š å‚è€ƒæ–‡çŒ®

- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)


**å‚è€ƒ**: SkyXZ 
